
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Audio-Visual Scene Understanding</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Organizers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CVPR 2021 Tutorial" />
<meta property="og:description" content="CVPR 2021 Tutorial" />
<link rel="canonical" href="https://audio-visual-scene-understanding.github.io/" />
<meta property="og:url" content="https://audio-visual-scene-understanding.github.io/" />
<meta property="og:site_name" content="Audio-Visual Scene Understanding" />
<script type="application/ld+json">
{"description":"CVPR 2021 Tutorial","@type":"WebSite","url":"https://audio-visual-scene-understanding.github.io/","name":"Audio-Visual Scene Understanding","headline":"Audio-Visual Scene Understanding"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="/assets/css/style.css?v=6630701df1d052c81ea810d987f1f29fdd76c5ad">
    <link rel="stylesheet" href="/assets/mystyle.css">


  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Audio-Visual Scene Understanding</h1>
      <h2 class="project-tagline">CVPR 2021<br>Time: June 19 </h2>
      
      
    </section>

    <section class="main-content">
<!--       <h2 id="organizers">Organizers</h2>
 -->



<div class="containertext">
  <h2 style="text-align: center">Overview</h2>
    <p>Sight and hearing are two of the most important senses for human perception. From cognitive perspective, the visual and auditory information is actually slightly discrepant, but the percept is unified with multisensory integration. What’s more, when there are multiple input senses, human reactions usually perform more exactly or efficiently than single sense. Inspired by this, for computational models, our community has begun to explore marrying computer vision with audition, and targets to address some essential problems of audio-visual learning then further develops them into interesting and worthwhile tasks. In recent years, we were delighted to witness many developments in learning from both visual and auditory data.
    </p>
    <p>This tutorial aims to cover recent advances in audio-visual learning, from the neuroscience study of humans to the computation models of machine. For each research sub-topic, we will give a concrete introduction of the contained problems/tasks, and the current research progress as well as the open problems. We hope the audience, not only the graduate students but also the researchers new in this area, can benefit from this tutorial and learn the principle problems and cutting-edge approaches of audio-visual learning.
    </p>
    </p>
</div> 

<br>



<div class="container">
  <h2>Schedule</h2>

  <div class="tab ">
    <button class="tablinks">Time Zone:</button>
    <button id="default_button" class="tablinks" onclick="openCity(event, 'EST')">N. America (East)</button>
    <button class="tablinks" onclick="openCity(event, 'CST')">China/Singapore</button>
    <button class="tablinks" onclick="openCity(event, 'CET')">Europe (Central)</button>
  </div>



    <div id="EST" class="tabcontent" style="text-align: left;">
      <p><span class="announce_date">10:00 - 10:05 &nbsp;</span> <strong> Welcome </strong> </p>
      <p><span class="announce_date">10:05 - 10:55 &nbsp;</span> <strong> Di Hu </strong> </p>
      <p><span class="announce_date">10:55 - 11:45 &nbsp;</span><strong> Yapeng Tian </strong>  </p>
      <p><span class="announce_date">11:45 - 12:35 &nbsp;</span> <strong> Lele Chen </strong> </p>
      <p><span class="announce_date">12:35 - 12:50 &nbsp;</span> <strong> Coffee Break </strong></p>
      <p><span class="announce_date">12:50 - 13:40 &nbsp;</span><strong> Amir Zadeh </strong>  </p>
      <p><span class="announce_date">13:40 - 14:30 &nbsp;</span><strong> Zhiyao Duan </strong>  </p>
      <p><span class="announce_date">14:30 - 15:20 &nbsp;</span><strong> Ross K. Maddox </strong>  </p>
      <p><span class="announce_date">15:20 - 16:10 &nbsp;</span><strong> Chenliang Xu (TBD) </strong>  </p>
      <p><span class="announce_date">16:10 - 16:30 &nbsp;</span><strong> Q&A </strong>  </p>
      <p><span class="announce_date">16:30 - 16:40 &nbsp;</span><strong> Closing Remarks </strong>  </p>
    </div>

    <div id="CST" class="tabcontent" style="text-align: left;">
      <p><span class="announce_date">22:00 - 22:05 &nbsp;</span> <strong> Welcome </strong> </p>
      <p><span class="announce_date">22:05 - 22:55 &nbsp;</span> <strong> Di Hu </strong> </p>
      <p><span class="announce_date">22:55 - 23:45 &nbsp;</span><strong> Yapeng Tian </strong>  </p>
      <p><span class="announce_date">23:45 - 00:35 (Day+1) &nbsp;</span> <strong> Lele Chen </strong> </p>
      <p><span class="announce_date">00:35 - 00:50 (Day+1)  &nbsp;</span> <strong> Coffee Break </strong></p>
      <p><span class="announce_date">00:50 - 01:40 (Day+1)&nbsp;</span><strong> Amir Zadeh </strong>  </p>
      <p><span class="announce_date">01:40 - 02:30 (Day+1)&nbsp;</span><strong> Zhiyao Duan </strong>  </p>
      <p><span class="announce_date">02:30 - 03:20 (Day+1)&nbsp;</span><strong> Ross K. Maddox </strong>  </p>
      <p><span class="announce_date">03:20 - 04:10 (Day+1)&nbsp;</span><strong> Chenliang Xu (TBD) </strong>  </p>
      <p><span class="announce_date">04:10 - 04:30 (Day+1)&nbsp;</span><strong> Q&A </strong>  </p>
      <p><span class="announce_date">04:30 - 04:40 (Day+1) &nbsp;</span><strong> Closing Remarks </strong>  </p>
    </div>

    <div id="CET" class="tabcontent" style="text-align: left;">
      <p><span class="announce_date">16:00 - 16:05 &nbsp;</span> <strong> Welcome </strong> </p>
      <p><span class="announce_date">16:05 - 16:55 &nbsp;</span> <strong> Di Hu </strong> </p>
      <p><span class="announce_date">16:55 - 17:45 &nbsp;</span><strong> Yapeng Tian </strong>  </p>
      <p><span class="announce_date">17:45 - 18:35 &nbsp;</span> <strong> Lele Chen </strong> </p>
      <p><span class="announce_date">18:35 - 18:50 &nbsp;</span> <strong> Coffee Break </strong></p>
      <p><span class="announce_date">18:50 -19:40 &nbsp;</span><strong> Amir Zadeh </strong>  </p>
      <p><span class="announce_date">19:40 - 20:30 &nbsp;</span><strong> Zhiyao Duan </strong>  </p>
      <p><span class="announce_date">20:30 - 21:20 &nbsp;</span><strong> Ross K. Maddox </strong>  </p>
      <p><span class="announce_date">21:20 - 22:10 &nbsp;</span><strong> Chenliang Xu (TBD) </strong>  </p>
      <p><span class="announce_date">22:10 - 22:30 &nbsp;</span><strong> Q&A </strong>  </p>
      <p><span class="announce_date">22:30 - 22:40 &nbsp;</span><strong> Closing Remarks </strong>  </p>
    </div>
</div>
 

<br>

<div class="table-wrapper" style ="width:100%">
  <table class="alt">
      <tbody>
          <col width="25%">
          <col width="60%">
          <col width="15%">
          <tr>
              <td>Welcome</td>
              <td></td>
              <td><a href="https://www.youtube.com/playlist?list=PL53R9Jy9Cc0zdv9OqvJ5YsZH2-AMKo9gM" target="_blank"><i class="fa fa-youtube"></i> playlist</a></td>
          </tr>
          <tr>
              <td>Aäron van den Oord</td>
              <td>
                  <button type="button" class="collapsible">Next Challenges for Self-Supervised Learning</button>
                  <div class="collapsible_content"><p>TBD</p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=jJozjCG8Cqs" target="_blank"><i class="fa fa-youtube"></i> recording</a></td>
          </tr>
          <tr>
              <td>Paolo Favaro</td>
              <td>
                  <button type="button" class="collapsible">Perspectives on Unsupervised Representation Learning</button>
                  <div class="collapsible_content"><p>Unsupervised representation learning is becoming a practical and effective approach to avoid massive labeling of data. Recent methods based on self-supervised learning have shown remarkable progress and are now able to build features that are competitive with features built through supervised learning. However, it is still unclear why some methods perform better than others. I will give an overview of methods that have been proposed in the literature and provide some analysis to try and understand what factors might be importantin the design of the next generation of self-supervised learning methods. </p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=APwHDZZcLuY" target="_blank"><i class="fa fa-youtube"></i> recording</a><br><a href="assets/sslwin_pf.pdf"><i class="fa fa-file-pdf-o"></i> slides</a></td>
          </tr>
          <tr>
              <td>Carl Doersch</td>
              <td>
                  <button type="button" class="collapsible">Learning and transferring visual representations with few labels</button>
                  <div class="collapsible_content"><p>When encountering novelty, like new tasks and new domains, current visual representations struggle to transfer knowledge if trained on standard tasks like ImageNet classification.  This talk explores how to build representations which better capture the visual world, and transfer better to new tasks.  I'll first discuss Bootstrap Your Own Latent (BYOL), a self-supervised representation learning algorithm based on the 'contrastive' method SimCLR.  BYOL outperforms its baseline without 'contrasting' its predictions with any 'negative' data; I'll provide a new perspective on why this avoids a collapse to a trivial solution.  Second, I'll present CrossTransformers, which achieves state-of-the-art few-shot fine-grained recognition on Meta-Dataset, via a self-supervised representation that's aware of spatial correspondence.</p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=RWCc0nZOSBw" target="_blank"><i class="fa fa-youtube"></i> recording</a><br><a href="assets/sslwin_cd.pdf"><i class="fa fa-file-pdf-o"></i> slides</a></td>
          </tr>
          <tr>
              <td>Andrew Zisserman</td>
              <td>
                  <button type="button" class="collapsible">Beyond Self-Supervised Representation Learning</button>
                  <div class="collapsible_content"><p>The talk will describe three phases of self-supervised learning. First, the `classical' phase, where the goal is semantic representation learning, e.g. training a network on ImageNet for an image representation. The second, `expansion' phase goes beyond single modality representation learning. The goals are more general and cover classical computer vision tasks, such as tracking and segmentation; and the data can be multi-modal such as video with audio. Tasks here include learning joint embeddings, learning to localize objects, and learning to transform videos into discrete objects. The final `uncurated' phase involves self-supervised learning from  uncurated data.</p></div>
              </td>
              <td><a href="assets/sslwin_az.pdf"><i class="fa fa-file-pdf-o"></i> slides</a></td>
          </tr>
          <tr>
              <td>Ishan Misra</td>
              <td>
                  <button type="button" class="collapsible">Multi-view invariance and grouping for self-supervised learning</button>
                  <div class="collapsible_content"><p>In this talk I will present our recent efforts in learning representation learning that can benefit semantic downstream tasks. Our methods build on two simple yet powerful insights - 1) The representation must be stable under different data augmentations or "views" of the data; 2) The representation must group together instances that co-occur in different views or modalities. I will show that these two insights can be applied to weakly supervised and self-supervised learning, to image, video, and audio data to learn highly performant representations. For example, these representations outperform weakly supervised representations trained on billions of images or millions of videos; can outperform ImageNet supervised pretraining on a variety of downstream tasks; and have led to state-of-the-art results on multiple benchmarks. These methods build upon prior work in clustering and contrastive methods for representation learning. I will conclude the talk by presenting shortcomings of our work and some preliminary thoughts on how they may be addressed.</p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=gbziPIn9uDI" target="_blank"><i class="fa fa-youtube"></i> recording</a><br><a href="assets/sslwin_im.pdf"><i class="fa fa-file-pdf-o"></i> slides</a></td>
          </tr>
          <tr>
              <td>Stella Yu</td>
              <td>
                  <button type="button" class="collapsible">Representation Learning beyond Instance Discrimination and Semantic Categorization</button>
                  <div class="collapsible_content"><p>Unsupervised representation learning has made great strides with invariant mapping and instance-level discrimination, as benchmarked by classification on common datasets.  However, these datasets are curated to be distinctive and class-balanced, whereas naturally collected data could be highly correlated within the class and long-tail distributed across classes.  The natural grouping of instances conflicts with the fundamental assumption of instance-level discrimination. Contrastive feature learning is thus unstable without grouping, whereas grouping without contrastive feature learning is easily trapped into degeneracy.  By integrating grouping into a discriminative metric learning framework, I will show that we can not only outperform the state-of-the-art on various classification, transfer learning, semi-supervised learning benchmarks with a much smaller (academic :-) compute, but also extend the goal of representation learning beyond semantic categorization.</p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=F5mt4z-w_Mk" target="_blank"><i class="fa fa-youtube"></i> recording</a></td>
          </tr>
          <tr>
              <td>Alexei (Alyosha) Efros</td>
              <td>
                  <button type="button" class="collapsible">Self-supervision as a path to a Post-dataset Era</button>
                  <div class="collapsible_content"><p>TBD</p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=iTbfEXFwDJc" target="_blank"><i class="fa fa-youtube"></i> recording</a></td>
          </tr>
          <tr>
              <td>Deepak Pathak</td>
              <td>
                  <button type="button" class="collapsible">Self-Supervision and Modularity: Cornerstones for Generalization in Embodied Agents</button>
                  <div class="collapsible_content"><p>TBD</p></div>
              </td>
              <td><a href="https://www.youtube.com/watch?v=fUMpC_hoedA" target="_blank"><i class="fa fa-youtube"></i> recording</a></td>
          </tr>
  </tbody>
</table>
</div>



<!--
<h2 id="time-and-location">Time and Location</h2>
<p>June 17, 2019. 9:00 am - 12:30 pm. Room 203A</p>

<h2 id="tutorial-schedule">Tutorial Schedule</h2>

<ul>
  <li>9:00 am - 9:10 am : Introduction (Nikhil Naik)</li>
  <li>9:10 am - 9:55 am : Few-shot meta-learning (Chelsea Finn)</li>
  <li>9:55 am - 10:40 am : Multi-task learning and meta-learning (Nitish Keskar)</li>
  <li>10:40 am - 11:00 am : Coffee Break</li>
  <li>11:00 am - 11:45 am: Neural Architecture Search (Nikhil Naik)</li>
  <li>11:45 am - 12:30 am: Bayesian Optimization and Meta-learning (Frank Hutter)</li>
</ul>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

  -->

  <div class="container">
    <h2>Organizers</h2>
      <div>
  
        <div class="instructor">
        <a href="https://dtaoo.github.io/"  target="_blank">
        <div class="instructorphoto"><img src="https://i.postimg.cc/ncrfsmC7/dihu.jpg" width="20%" hspace="2%">   </div>  
        <div>Di Hu<br><small>Renmin University of China</small></div></a>
  
        </div>
    
        <div class="instructor">
                <a href="http://yapengtian.org/"  target="_blank">
  
        <div class="instructorphoto"><img src="https://i.postimg.cc/NFfqk9D3/yapeng.jpg" width="20%" hspace="2%">  </div> 
        <div>Yapeng Tian<br><small>University of Rochester</small></div></a>
  
        </div>
  
        <div class="instructor">
                <a href="https://www.cs.rochester.edu/u/lchen63/"  target="_blank">
  
        <div class="instructorphoto"><img src="https://i.postimg.cc/L5CdC90n/Lele.jpg" width="20%" hspace="2%"> </div>
        <div>Lele Chen<br><small>University of Rochester</small></div></a>
  
        </div>
  
        <div class="instructor">
                <a href="https://www.amir-zadeh.com/"  target="_blank">
  
        <div class="instructorphoto"><img src="https://i.postimg.cc/85bgvBRd/Amir.jpg" width="20%" hspace="2%"></div>
        <div>Amir Zadeh<br><small>Carnegie Mellon University</small></div></a>
        </div>
  
        <div class="instructor">
                <a href="http://www2.ece.rochester.edu/~zduan/"  target="_blank">
  
        <div class="instructorphoto"><img src="https://i.postimg.cc/764ybQ2Y/duan.jpg" width="20%" hspace="2%">     </div>
        <div>Zhiyao Duan<br><small>University of Rochester</small></div></a>
        </div>
    
        <div class="instructor">
                <a href="https://www.urmc.rochester.edu/labs/maddox.aspx"  target="_blank">
  
        <div class="instructorphoto"><img src="https://i.postimg.cc/HLXgYCjh/ross.jpg" width="20%" hspace="2%"></div>   
        <div>Ross K. Maddox<br><small>University of Rochester</small></div></a>
  
      </div>
  
      <div class="instructor">
        <a href="https://www.cs.rochester.edu/~cxu22/"  target="_blank">
  
      <div class="instructorphoto"><img src="https://i.postimg.cc/1RJZ5B5s/xu.jpg" width="20%" hspace="2%"></div>   
    <div>Chenliang Xu<br><small>University of Rochester</small></div></a>
      </div>
  
  
  
  
  
  </div>

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>


  <script>
    //openCity(event, 'PST');
    document.getElementById("default_button").click(); // Click on the checkbox
    function openCity(evt, cityName) {
      // Declare all variables
      var i, tabcontent, tablinks;
    
      // Get all elements with class="tabcontent" and hide them
      tabcontent = document.getElementsByClassName("tabcontent");
      for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
      }
    
      // Get all elements with class="tablinks" and remove the class "active"
      tablinks = document.getElementsByClassName("tablinks");
      for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
      }
    
      // Show the current tab, and add an "active" class to the button that opened the tab
      document.getElementById(cityName).style.display = "block";
      evt.currentTarget.className += " active";
    }
    
    </script>
  </body>
</html>
